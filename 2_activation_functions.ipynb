{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.activation functions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43pFzkY6ZBiK"
      },
      "source": [
        "**What is an activation function and why to use them?**\n",
        "\n",
        "Definition of activation function:- Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it.\n",
        "\n",
        "The purpose of the activation function is to introduce non-linearity into the output of a neuron\n",
        "\n",
        "activation functions in TF.Keras https://www.tensorflow.org/api_docs/python/tf/keras/activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97STbEFJZ0i-"
      },
      "source": [
        "**Implementing few activation functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqvKAFuXYvWP",
        "outputId": "cd2be23b-e60d-4fc0-f4d8-2403858643ae"
      },
      "source": [
        "#implementing sigmoid \n",
        "#The sigmoid function always returns a value between 0 and 1.\n",
        "#sigmoid has an issue of vanishing gradient (https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
        "#S(x)={1}/{1+e^{-x}}\n",
        "\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return (1/(1+ math.exp(-x)))\n",
        "\n",
        "#Examples\n",
        "\n",
        "print(sigmoid(100))\n",
        "print(sigmoid(1))\n",
        "print(sigmoid(-10))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.7310585786300049\n",
            "4.5397868702434395e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5d9vr1dYvX2",
        "outputId": "9ce74417-7139-4cdb-bc90-9fd88202b663"
      },
      "source": [
        "#implementing tanh \n",
        "#The sigmoid function always returns a value between -1 and 1.\n",
        "#tanh has an issue of vanishing gradient (https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
        "#S(x)={e^{x} - e^{-x}} / {e^{x} + e^{-x}}\n",
        "\n",
        "def tanh(x):\n",
        "  return {( math.exp(x) -  math.exp(-x))/ (math.exp(x) +  math.exp(-x))}\n",
        "\n",
        "#example\n",
        "print(tanh(100))\n",
        "print(tanh(-10))\n",
        "print(tanh(1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1.0}\n",
            "{-0.9999999958776926}\n",
            "{0.7615941559557649}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTKJkPvjYvbE",
        "outputId": "8262c01e-e8c3-4917-c9b0-9a5560a81eaf"
      },
      "source": [
        "#implementing Relu\n",
        "#all negitive values return 0 and positive value returns the same value back\n",
        "\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "\n",
        "#example\n",
        "print(relu(10))\n",
        "print(relu(-5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATONQ5gwe0Za",
        "outputId": "1c7cd0f1-1c96-44b2-ca82-037043c46d03"
      },
      "source": [
        "#implementing Leaky relu\n",
        "#for all negetive values it returns 0.1*x\n",
        "#for all posetive values it returns the same values \n",
        "\n",
        "def leaky_relu(x):\n",
        "  return max(0.1*x,x)\n",
        "\n",
        "#example\n",
        "print(leaky_relu(100))\n",
        "print(leaky_relu(-100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "-10.0\n"
          ]
        }
      ]
    }
  ]
}